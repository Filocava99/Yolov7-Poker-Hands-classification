{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "844cf00f-357b-4e14-8511-ad8eb165c6c8",
   "metadata": {},
   "source": [
    "# Introduction to Yolo\n",
    "YOLO is a popular object detection algorithm that can recognize and localize multiple objects within an image or video in real-time. The YOLO network uses a single convolutional neural network (CNN) architecture to simultaneously perform object detection and classification.\n",
    "\n",
    "The YOLO network divides an input image into a grid of cells and predicts bounding boxes, objectness scores, and class probabilities for each cell. Each bounding box is represented by five values: the x and y coordinates of the box's center, the box's width and height, and a confidence score that indicates the probability that the box contains an object. The objectness score indicates the probability that an object is present in the box, and the class probabilities represent the probabilities of the box containing objects of different classes.\n",
    "\n",
    "The YOLO network uses a variant of the Darknet architecture, which consists of 53 convolutional layers followed by several fully connected layers. The network architecture is designed to optimize the trade-off between accuracy and speed, allowing the algorithm to achieve real-time performance even on low-powered devices.\n",
    "\n",
    "Training the YOLO network involves using a large dataset of annotated images to optimize the network's parameters through backpropagation. During training, the network learns to predict bounding boxes and class probabilities that accurately match the ground truth annotations.\n",
    "\n",
    "Overall, YOLO is a highly effective and efficient object detection algorithm that has been widely used in a variety of applications, including autonomous vehicles, surveillance systems, and image and video analysis.\n",
    "\n",
    "## Yolo 7\n",
    "YOLOv7 provides a greatly improved real-time object detection accuracy without increasing the inference costs. When compared to other known object detectors, YOLOv7 can effectively reduce about 40% parameters and 50% computation of state-of-the-art real-time object detections, and achieve faster inference speed and higher detection accuracy. In general, YOLOv7 provides a faster and stronger network architecture that provides a more effective feature integration method, more accurate object detection performance, a more robust loss function, and an increased label assignment and model training efficiency. As a result, YOLOv7 requires several times cheaper computing hardware than other deep learning models. It can be trained much faster on small datasets without any pre-trained weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729a6a64-2cd3-4646-b7cd-72786dd1b373",
   "metadata": {},
   "source": [
    "## Clone Yolo v7 repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5014488d-7fd6-478c-9479-21db1f2790e1",
   "metadata": {},
   "source": [
    "The repository being cloned is a fork of the [Yolov7](https://github.com/WongKinYiu/yolov7) repository with customized settings.\n",
    "The following settings have been customized for the purpose of this project:\n",
    "- `cfg/training/yolov7.yaml` update the classes number to 52\n",
    "- `data/coco.yaml`: changed the path of the datasets and updated the number and value of the classes\n",
    "- `data/data.yaml`: it's a copy of the coco.yaml file; such copy was created because using the original file would trigger some tasks specific for the coco dataset\n",
    "- `requirements.txt`: replace the default torch version with the CUDA enabled one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ec2f44-3412-4707-8043-9de2c680b498",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Filocava99/Yolov7-Poker-Hands-classification.git yolov7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c2df42-1948-474c-83a7-513a86fc4504",
   "metadata": {},
   "source": [
    "## Libraries import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa97b2f3-e18a-48b1-93c0-daf92cd2af6d",
   "metadata": {},
   "source": [
    "Before training the model all the required modules have to be installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd28984-7aeb-44ff-8dc1-7c7d2d3eb91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r .//yolov7//requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f3a4d4-9952-4bc9-ac2a-1c510c4e1a26",
   "metadata": {},
   "source": [
    "To install pytorch with CUDA support run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b66bfb1-819b-4cb7-a92c-0e4346761776",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e48c37a9-54bf-434b-bf8a-cbb307e36410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, io, shutil, os, torch, pycocotools, yaml\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147a9ae0-8af0-4af5-a2e7-61deed678c14",
   "metadata": {},
   "source": [
    "Downloading the compiled Yolo v7 model and its less complex version (not used in the final/production version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be5efffb-fd7b-4302-908c-1f59e2d9364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_v7 = \"https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7_training.pt\"\n",
    "yolo_v7_tiny = \"https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-tiny.pt\"\n",
    "r = requests.get(yolo_v7)\n",
    "with open('./yolov7/yolov7.pt', \"wb\") as f:\n",
    "    f.write(r.content)\n",
    "r = requests.get(yolo_v7_tiny)\n",
    "with open('./yolov7/yolov7_tiny.pt', \"wb\") as f:\n",
    "    f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f95d79c-db10-46bb-a63e-a3a1ddf74564",
   "metadata": {},
   "source": [
    "## Dataset import\n",
    "The following cell will download the dataset from a Google drive url. In case the url has expired you can generate a new one from the commented Kaggle link (second line of code).  \n",
    "After the dataset has been downloaded, all the classes described in the `data.yaml` file will be parsed for later use and then printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86657a9d-7066-46b3-9bf9-e3fd9bf52371",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total classes: 52\n",
      "Classes: ['10c', '10d', '10h', '10s', '2c', '2d', '2h', '2s', '3c', '3d', '3h', '3s', '4c', '4d', '4h', '4s', '5c', '5d', '5h', '5s', '6c', '6d', '6h', '6s', '7c', '7d', '7h', '7s', '8c', '8d', '8h', '8s', '9c', '9d', '9h', '9s', 'Ac', 'Ad', 'Ah', 'As', 'Jc', 'Jd', 'Jh', 'Js', 'Kc', 'Kd', 'Kh', 'Ks', 'Qc', 'Qd', 'Qh', 'Qs']\n"
     ]
    }
   ],
   "source": [
    "obj_detection_dataset_url = \"https://storage.googleapis.com/kaggle-data-sets/1043676/3161771/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20230511%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20230511T201858Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=96cccaa097f4ee5911ef388fc1c5dddfdc82022539f4cbbc2d81bd012cbeff718c7fe226e1cba6f5992fcf266895028b45e8467790903cf0a0de70836dcbc7c80b075b1162243cb78cddd22d7c16c4111c95c06c862f9b6ba0a6b26b546d1b8947f537a7ff5ccc37b5cf33872a3338386ca922bfb0000dce2b083201a6afc92b50e8deac96a0ead798488c2c964b1887f0505e16fab6134962182b4f614f4e6065f91f14788a2768b14a5f326ad3603e2d1b66deb8b119e7646c8dde6724c27fc775beda4892a47f0c2b75f1db50bd92b48d5b8707f1c78ab766a0ece27e03436f358f9f7d46ab8fc464037b35fec66fd7ee0488d5ae2d5399158fc5a54147d1\"\n",
    "#\"https://drive.google.com/uc?export=download&id=1GOJ9REocmncYz5Fc_bXgF3vb313EJrsh\"\n",
    "#\"https://www.kaggle.com/datasets/andy8744/playing-cards-object-detection-dataset/download?datasetVersionNumber=4\"\n",
    "\n",
    "if(os.path.exists(\"./yolov7/dataset\") == False):\n",
    "    r = requests.get(obj_detection_dataset_url)\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    z.extractall(\"./yolov7/dataset\")\n",
    "    \n",
    "from yaml.loader import SafeLoader\n",
    "classes = []\n",
    "# Open the file and load the file\n",
    "with open('./yolov7/dataset/data.yaml') as f:\n",
    "    classes = yaml.load(f, Loader=SafeLoader)[\"names\"]\n",
    "print(\"Total classes: \" + str(len(classes)))\n",
    "print(\"Classes: \" + str(classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7747fb02-583f-40e5-914d-2bdde0bec649",
   "metadata": {},
   "source": [
    "### Important\n",
    "> The following steps can be skipped if you want to use the already trained model, otherwise be aware that the training phase can require up to 4 hours to complete (tested on an RTX 4080)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c67c9e5-e72f-4ca6-8035-db5bbcbfc191",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Remove roboflow junk from file names\n",
    "The dataset creator used roboflow to generate both the labels and the images, so we have to strip such files from unneeded information, especially in their files name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f4aea5a-f965-4211-beb4-c0e604085203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "# remove roboflow extra junk\n",
    "\n",
    "count = 0\n",
    "for i in sorted(os.listdir('./yolov7/dataset/train/labels')):\n",
    "    if count >=3:\n",
    "        count = 0\n",
    "    count += 1\n",
    "    if i[0] == '.':\n",
    "        continue\n",
    "    j = i.split('_')\n",
    "    dict1 = {1:'a', 2:'b', 3:'c'}\n",
    "    source = './yolov7/dataset/train/labels/'+i\n",
    "    dest = './yolov7/dataset/train/labels/'+j[0]+dict1[count]+'.txt'\n",
    "    os.rename(source, dest)\n",
    "    \n",
    "count = 0\n",
    "for i in sorted(os.listdir('./yolov7/dataset/train/images')):\n",
    "    if count >=3:\n",
    "        count = 0\n",
    "    count += 1\n",
    "    if i[0] == '.':\n",
    "        continue\n",
    "    j = i.split('_')\n",
    "    dict1 = {1:'a', 2:'b', 3:'c'}\n",
    "    source = './yolov7/dataset/train/images/'+i\n",
    "    dest = './yolov7/dataset/train/images/'+j[0]+dict1[count]+'.jpg'\n",
    "    os.rename(source, dest)\n",
    "    \n",
    "for i in sorted(os.listdir('./yolov7/dataset/valid/labels')):\n",
    "    if i[0] == '.':\n",
    "        continue\n",
    "    j = i.split('_')\n",
    "    source = './yolov7/dataset/valid/labels/'+i\n",
    "    dest = './yolov7/dataset/valid/labels/'+j[0]+'.txt'\n",
    "    os.rename(source, dest)\n",
    "    \n",
    "for i in sorted(os.listdir('./yolov7/dataset/valid/images')):\n",
    "    if i[0] == '.':\n",
    "        continue\n",
    "    j = i.split('_')\n",
    "    source = './yolov7/dataset/valid/images/'+i\n",
    "    dest = './yolov7/dataset/valid/images/'+j[0]+'.jpg'\n",
    "    os.rename(source, dest)\n",
    "for i in sorted(os.listdir('./yolov7/dataset/test/labels')):\n",
    "    if i[0] == '.':\n",
    "        continue\n",
    "    j = i.split('_')\n",
    "    source = './yolov7/dataset/test/labels/'+i\n",
    "    dest = './yolov7/dataset/test/labels/'+j[0]+'.txt'\n",
    "    os.rename(source, dest)\n",
    "    \n",
    "for i in sorted(os.listdir('./yolov7/dataset/test/images')):\n",
    "    if i[0] == '.':\n",
    "        continue\n",
    "    j = i.split('_')\n",
    "    source = './yolov7/dataset/test/images/'+i\n",
    "    dest = './yolov7/dataset/test/images/'+j[0]+'.jpg'\n",
    "    os.rename(source, dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425ea89f-7121-4407-969e-fb35571a4f27",
   "metadata": {},
   "source": [
    "## Train the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d85396f-34f4-465c-9b51-22aeb4ccda80",
   "metadata": {},
   "source": [
    "To train the dataset we use the following parameters:\n",
    "- workers: 8, adjust to the number of CPU cores\n",
    "- device: 0, where 0 is the id of the GPU\n",
    "- batch size: 32\n",
    "- epochs: 100\n",
    "- img: 416 416, the size of the images of the dataset\n",
    "- hyp: data/hyp.scratch.custom.yaml, the file containing the hyperparameters to use\n",
    "- name: yolov7-custom, the name to use to save the trained model\n",
    "- weights: yolov7.pt, the weights to start from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b651245-722c-461d-8131-307bc6f32741",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python .//yolov7//train.py --workers 8 --device 0 --batch-size 32 --epochs 100 --img 416 416 --hyp data//hyp.scratch.custom.yaml --name yolov7-custom --weights yolov7.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c01191-8285-4185-9150-e0aee9100de2",
   "metadata": {},
   "source": [
    "After the training we can run the model to detect the cards on different sources. In the first case we are providing a video while in the second case we are using the webcam feed stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20eaf8a-ff3d-48a9-9455-d5278f6fd843",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yt-dlp --format mp4 \"https://youtu.be/koxA7TqdNWk\"\n",
    "!python .//yolov7//detect.py --weights .//yolov7//runs//train//yolov7-custom//weights//best.pt --conf 0.5 --img-size 1280 --source \"videotest [koxA7TqdNWk].mp4\" --view-img --no-trace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dd012d-8581-47c2-8aa2-68cb52cbd917",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python .//yolov7//detect.py --weights .//yolov7//runs//train//yolov7-custom//weights//best.pt --conf 0.5 --img-size 1280 --source 0 --view-img --no-trace "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ec12c2-8ee9-4da1-8470-ed0eb1db8cf4",
   "metadata": {},
   "source": [
    "## Test evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d07daab-b14e-4333-ab38-24d94bf01d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a3ebedebf524e02a69e624fea8f29d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\t`\\x00\\x00\\x04\\xb0\\x08\\x06\\x00\\x00\\x00\\x7f\\xa4q\\x81\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the image\n",
    "image_path = './yolov7/runs/train/yolov7-custom/results.png'\n",
    "with open(image_path, 'rb') as f:\n",
    "    image = f.read()\n",
    "\n",
    "# Create the image widget\n",
    "image_widget = widgets.Image(\n",
    "    value=image, \n",
    "    format='png',\n",
    "    width=1920\n",
    ")\n",
    "display(image_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d1f0c8-728e-4b72-8cef-2604c72fd7b7",
   "metadata": {},
   "source": [
    "The graphs shown in the previous cell were generated directly from the implementation of Yolo 7 that was used. Of particular interest are the precision, recall, and mAP graphs, both with an IoU threshold of 0.5 and 0.95.\n",
    "\n",
    "The model converged around the 30th epoch, but we continued training up to 100 epochs to verify any potential improvements. The mAP improved slightly, but there were no issues of overfitting, so overall the subsequent epochs were not considered useless."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551f8513-240d-4f53-bee0-d1879a7463c5",
   "metadata": {},
   "source": [
    "## Score evaluation algorithm\n",
    "This algorithm will return a tuple of two elements, the first element is an integer between 1 and 10 representing the point value of the hand and the second element is the highest card in case of tie-breaker.\n",
    "The point values are:\n",
    "- 10 for Straight Flush\n",
    "- 8 for Four of a Kind\n",
    "- 7 for Full House\n",
    "- 6 for Flush\n",
    "- 5 for Straight\n",
    "- 4 for Three of a Kind\n",
    "- 3 for Two Pairs\n",
    "- 2 for One Pair\n",
    "- 1 for High Card\n",
    "\n",
    "The expected input for this algorithm is a list of five strings, where each string represents a poker card in the format of \"value suit\". The value can be one of the following: '2', '3', '4', '5', '6', '7', '8', '9', 'T', 'J', 'Q', 'K', 'A'. The suit can be one of the following: 'C' for Clubs, 'D' for Diamonds, 'H' for Hearts, 'S' for Spades.\n",
    "```python\n",
    "cards = [\"2 H\", \"3 H\", \"4 H\", \"5 H\", \"6 H\"]\n",
    "poker_hand(cards)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76a014c5-ee57-4a9e-a674-1cde51414e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "points = {\n",
    "    10: \"Straight flush\",\n",
    "    8: \"Four of a kind\",\n",
    "    7: \"Full house\",\n",
    "    6: \"Flush\",\n",
    "    5: \"Straight\",\n",
    "    4: \"Three of a kind\",\n",
    "    3: \"Two pairs\",\n",
    "    2: \"One pair\",\n",
    "    1: \"High card\"\n",
    "}\n",
    "\n",
    "def poker_hand(cards):\n",
    "    # Create a dictionary to store the count of each card value\n",
    "    card_count = collections.defaultdict(int)\n",
    "    for card in cards:\n",
    "        value = card[0]\n",
    "        if value == 'T':\n",
    "            value = '10'\n",
    "        elif value == 'J':\n",
    "            value = '11'\n",
    "        elif value == 'Q':\n",
    "            value = '12'\n",
    "        elif value == 'K':\n",
    "            value = '13'\n",
    "        elif value == 'A':\n",
    "            value = '14'\n",
    "        card_count[value] += 1\n",
    "    # Check for a flush (all cards are the same suit)\n",
    "    flush = all(card[2] == cards[0][2] for card in cards)\n",
    "    # Check for a straight (cards are in sequential order)\n",
    "    straight = (max([int(x) for x in card_count.keys()]) - min([int(x) for x in card_count.keys()]) == 4) and (len(card_count) == 5)\n",
    "    # Check for a straight flush (flush and straight)\n",
    "    straight_flush = flush and straight\n",
    "    # Check for four of a kind\n",
    "    four_of_a_kind = any(count == 4 for count in card_count.values())\n",
    "    # Check for three of a kind\n",
    "    three_of_a_kind = any(count == 3 for count in card_count.values())\n",
    "    # Check for a full house (three of a kind and one pair)\n",
    "    full_house = three_of_a_kind and any(count == 2 for count in card_count.values())\n",
    "    # Check for two pairs\n",
    "    two_pairs = len([count for count in card_count.values() if count == 2]) == 2\n",
    "    # Check for one pair\n",
    "    one_pair = len([count for count in card_count.values() if count == 2]) == 1\n",
    "    if straight_flush:\n",
    "        return (points[10], max(card_count.keys()))\n",
    "    elif four_of_a_kind:\n",
    "        return (points[8], max(card_count, key=lambda x: card_count[x]))\n",
    "    elif full_house:\n",
    "        return (points[7], max(card_count, key=lambda x: card_count[x]))\n",
    "    elif flush:\n",
    "        return (points[6], max(card_count.keys()))\n",
    "    elif straight:\n",
    "        return (points[5], max(card_count.keys()))\n",
    "    elif three_of_a_kind:\n",
    "        return (points[4], max(card_count, key=lambda x: card_count[x]))\n",
    "    elif two_pairs:\n",
    "        return (points[3], max(card_count, key=lambda x: card_count[x]))\n",
    "    elif one_pair:\n",
    "        return (points[2], max(card_count, key=lambda x: card_count[x]))\n",
    "    else:\n",
    "        return (points[1], max(card_count.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f6875d-3135-48f5-b7e2-43a88f823caa",
   "metadata": {},
   "source": [
    "You can select a photo of a poker hand to test out the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2577b4ab-c158-43b6-a2eb-e8bd5fcd55a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ae99dcfc004570b63840a4fa41def4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FileUpload(value=(), description='Upload'),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "uploader = widgets.FileUpload(multiple=False)\n",
    "display(widgets.HBox([uploader]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "138c7310-b49c-4897-9d37-e67e88426175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e15ab6a194d44abb2413ec36d074486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00H\\x00H\\x00\\x00\\xff\\xe2\\x02(ICC_PROFILE\\x00\\x01\\x…"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileName = list(uploader.value[0].items())[0][1]\n",
    "content = list(uploader.value[0].items())[3][1]\n",
    "widgets.Image(value=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45800b46-a8d3-4842-ba94-8a8c0ac8edc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Width: 720\n",
      "Height: 1280\n",
      "If you want to use the webcam feed instead of the uploaded image please check the following checkbox:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e54c7321da401d9b5ebc2cd677bb41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Use webcam')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "decoded = cv2.imdecode(np.frombuffer(content, np.uint8), -1)\n",
    "width = decoded.shape[1]\n",
    "height = decoded.shape[0]\n",
    "max_size = max(width, height)\n",
    "print(\"Width: \" + str(width))\n",
    "print(\"Height: \" + str(height))\n",
    "print(\"If you want to use the webcam feed instead of the uploaded image please check the following checkbox:\")\n",
    "checkbox = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Use webcam',\n",
    "    disabled=False\n",
    ")\n",
    "display(checkbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "099e7a7a-ef13-4e60-a535-7a5267e82c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(weights=['.//yolov7//runs//train//yolov7-custom//weights//best.pt'], source='.//runs//detect//custom//test.jpg', img_size=1280, conf_thres=0.5, iou_thres=0.45, device='', view_img=True, save_txt=True, save_conf=False, nosave=False, classes=None, agnostic_nms=False, augment=False, update=False, project='runs/detect', name='custom', exist_ok=True, no_trace=False)\n",
      "Fusing layers... \n",
      " Convert model to Traced-model... \n",
      " traced_script_module saved! \n",
      " model is traced! \n",
      "\n",
      "1 5d, 1 6d, 1 7h, 1 8d, 1 9c, Done. (25.0ms) Inference, (4.5ms) NMS\n",
      " The image with the result is saved in: runs\\detect\\custom\\test.jpg\n",
      "Done. (0.213s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOR  2023-5-11 torch 1.13.1+cu116 CUDA:0 (NVIDIA GeForce RTX 4080, 16375.375MB)\n",
      "\n",
      "C:\\Users\\filip\\anaconda3\\envs\\visione\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3191.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Model Summary: 200 layers, 6144193 parameters, 0 gradients, 13.5 GFLOPS\n"
     ]
    }
   ],
   "source": [
    "if(os.path.exists(\"./runs/detect/custom\")):\n",
    "    shutil.rmtree(\"./runs/detect/custom\")\n",
    "os.makedirs(\"./runs/detect/custom\")\n",
    "with open(\"./runs/detect/custom/test.jpg\", \"wb\") as f:\n",
    "    f.write(content)\n",
    "if(checkbox.value):\n",
    "    !python .//yolov7//detect.py --weights .//yolov7//runs//train//yolov7-custom//weights//best.pt --conf 0.5 --img-size 1280 --source 0 --view-img --no-trace \n",
    "else:\n",
    "    !python .//yolov7//detect.py --weights .//yolov7//runs//train//yolov7-custom//weights//best.pt --save-txt --conf 0.5 --img-size {max_size} --source .//runs//detect//custom//test.jpg --name custom --exist-ok --view-img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a6c8759-ff2b-44fc-90b1-a987b50502ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8 D', '7 H', '5 D', '9 C', '6 D']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./runs/detect/custom/labels/test.txt\", delim_whitespace=True, header=None)\n",
    "objs = []\n",
    "for index, row in df.iterrows():\n",
    "    name = str(classes[int(row[0])])\n",
    "    name = name[0] + \" \" + str.upper(name[1])\n",
    "    if((name in objs) == False):\n",
    "        objs.append(name)\n",
    "print(objs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23b080c6-dc45-4e4c-8a4b-4817de2e4a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Point: Straight\n",
      "Highest card value: 9\n"
     ]
    }
   ],
   "source": [
    "hand = poker_hand(objs)\n",
    "print(\"Point: \" + hand[0])\n",
    "print(\"Highest card value: \" + str(hand[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59797bfd-6da5-4313-b06a-16de64f022cb",
   "metadata": {},
   "source": [
    "## Final thoughs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26316b7-e49c-4cd5-947d-aef03b1703ac",
   "metadata": {},
   "source": [
    "The Yolo v7 model outperfomed the SSD model reaching a mAP of over 95%, while SSD only topped 5%. There are still a few issues, probably related to the dataset used; in particular, the model struggles to detect the values of the cards when the photo has been taken to close to them (some background has to be visible for the model to detect cards). Another issue, which is instead caused by the scoring algorithm, is that the same card may be counted twice when all of its corners are uncovered; as a design implementation I decided not to handle it, to better showcase how the model predictions work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
